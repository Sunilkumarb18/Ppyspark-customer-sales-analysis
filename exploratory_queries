1. Which customers increased their spending by at least 50% in the last 3 months compared to the previous 3 months

Query:
--------
from pyspark.sql.window import Window
wn_spec = Window.partitionBy("customer_id").orderBy("sale_date").rowsBetween(-2,0)
rn = sum("sale_amount").over(wn_spec)
df_roll = s.withColumn("rolling sum", rn)
df_roll.display()



2. Which customers increased their spending by at least 50% in the last 3 months compared to the previous 3 months

Query:
--------
from pyspark.sql.window import Window
wn_spec = Window.partitionBy("customer_id").orderBy("sale_date").rowsBetween(-2,0)
rn = sum("sale_amount").over(wn_spec)
df_roll = s.withColumn("rolling sum", rn)
df_roll.display()



3. What are the top 5 customers by total spend each month

Query:
-------
from pyspark.sql.functions import col,sum, date_format, row_number
from pyspark.sql.window import Window
df_mnt = s.withColumn("mnt",date_format("sale_date","MM"))
m = df_mnt.alias("m")
df_top5 = c.join(m, on ="customer_id", how="inner").groupBy("first_name","mnt").agg(sum(col("sale_amount")).alias("Tot Amount")).orderBy(col("Tot Amount").desc())
wn_spec = Window.partitionBy("mnt").orderBy(col("Tot Amount").desc())
rn = row_number().over(wn_spec)
df_rnk = df_top5.withColumn("rnk",rn)
df_rnk.filter(col("rnk")<6).drop("rnk").display()


4. Which products have consistently increased in sales over the last 6 months

Query:
-------
from pyspark.sql.functions import lag, when,sum
df_last_6 = p.join(m, on="product_id", how="inner").groupBy("product_name","mnt").agg(sum("sale_amount").alias("Tot Amount"))
df_m = df_last_6.filter(col("mnt")<7).orderBy("product_name","mnt")
wn_spec = Window.partitionBy("product_name").orderBy("mnt")
lg = lag("Tot Amount").over(wn_spec)
df_lag = df_m.withColumn("lag Diff",when((col("Tot Amount")-lg)>0, 1).otherwise(0))
df_lag.groupBy("product_name").agg(sum("lag Diff").alias("val")).filter(col("val")==5).display()


5. What is the average time between purchases per customer

Query:
-------
from pyspark.sql.functions import datediff, avg
df_avg_time = s.join(c, on="customer_id", how="inner")
wn_spec = Window.partitionBy("customer_id").orderBy("sale_date")
lg = lag("sale_date",1).over(wn_spec)
df_fn = df_avg_time.withColumn("diff", datediff(col("sale_date"),lg))
df1 = df_fn.select("customer_id","first_name","diff").fillna(0)
df1.groupBy("customer_id").agg(avg("diff")).display()

6. What is the average order value (AOV) by customer segment or region

Query:
------
from pyspark.sql.functions import sum,count
df_avg_val = s.agg(sum("Sale_amount")/count("sale_id"))
df_avg_val.display()

7.Year-over-Year Growth by Product Category

Query:
-------
from pyspark.sql.functions import sum,date_format,lag,col,round
from pyspark.sql.window import Window
df_mm = s.withColumn("year", date_format("sale_date","yyyy"))
m = df_mm.alias("m")
df_yoy = p.join(m, on="product_id", how="inner").groupBy("category","year").agg(sum("sale_amount").alias("Tot Amt")).orderBy("category","year")
wn_spec = Window.partitionBy("category").orderBy("year")
rn = lag("Tot Amt", 1).over(wn_spec)
df_lag = df_yoy.withColumn("prev_tot", rn)
df_per = df_lag.withColumn("val", round((col("prev_tot")-col("Tot Amt"))/col("Tot Amt")*100,2))
df_per.fillna(0).display()

8.Identify Customers with Increasing Monthly Spend

Query:
-------
from pyspark.sql.functions import date_format,sum,lag,when,col
from pyspark.sql.window import Window
df_mon = s.withColumn("mnt",date_format("sale_date","MM"))
s=df_mon.alias("m")
df_fn = s.join(c,on="customer_id", how="inner").groupBy("first_name","mnt").agg(sum("sale_amount").alias("tot")).orderBy("first_name","mnt")
wn_spec = Window.partitionBy("first_name").orderBy("mnt")
rn = lag("tot").over(wn_spec)
df_fn2 = df_fn.withColumn("val", when(col("tot")-rn>0,1).otherwise(0))
df_fn2.groupBy("first_name").agg(sum("val").alias("cnt")).filter(col("cnt")==11).display()

9. Repeat vs One-Time Customers

Query:
---------
from pyspark.sql.functions import count,col
df_one = s.groupBy("customer_id").agg(count("customer_id").alias("One-Time")).filter(col("One-Time")==1)
df_one.agg(count("customer_id")).display()
df_repeat = s.groupBy("customer_id").agg(count("customer_id").alias("Repeat")).filter(col("Repeat")==2)
df_repeat.agg(count("customer_id")).display()

10. Sales Velocity per Product-Calculate average number of sales per day for each product.

Query:
--------
from pyspark.sql.functions import count, countDistinct, col
product_sales = s.groupBy("product_id").agg(
    count("sale_id").alias("total_sales"),
    countDistinct("sale_date").alias("active_days")
)
product_sales = product_sales.withColumn(
    "avg_sales_per_day",
    col("total_sales") / col("active_days")
)

product_sales.select("product_id", "avg_sales_per_day").display()



